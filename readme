# Weather Agent POC

This repository contains a minimal FastAPI backend that demonstrates how a natural-language prompt can be interpreted by an agent, translated into weather API calls, and rendered into a downloadable PDF artifact.

## Running locally

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
uvicorn app.main:app --reload
```

Then call the API:

```bash
curl -X POST http://localhost:8000/api/weather-report \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Give me a business summary of temperature and rain in Austin next week"}' \
  --output weather.pdf
```

The API returns a PDF compiled from live Open-Meteo data.

## Enabling the free Hugging Face LLM

The backend can call Hugging Face's router to interpret prompts and author narratives.

1. Create a [Hugging Face access token](https://huggingface.co/settings/tokens) with `inference` scope.
2. Export the token and optional overrides before running the API:

   ```bash
   export HF_TOKEN="hf_..."
   export LLM_PROVIDER=huggingface
   export LLM_MODEL_INTERPRETER="CohereLabs/aya-expanse-32b:cohere"
   export LLM_MODEL_NARRATIVE="CohereLabs/aya-expanse-32b:cohere"
   ```

3. Start Uvicorn as shown above. When no token is provided the service gracefully falls back to the rule-based interpreter and summary generator.
